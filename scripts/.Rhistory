)
## analyze tunning results ----
show_best(
rf_tune_results,
metric = "rmse",
n = 5
)
autoplot(rf_tune_results)
best_rf_params <- select_best(
rf_tune_results,
metric = "rmse"
)
best_rf_params
## finalize workflow ----
final_rf_workflow <- finalize_workflow(
rf_workflow,
best_rf_params
)
final_rf_model_fit <- fit(
final_rf_workflow,
data = train_raw_data
)
## evaluate final model ----
processed_test_data_for_eval <- prep(
lag_recipe,
training = train_raw_data
) %>%
bake(new_data = test_raw_data)
augment_test_data <- augment(
final_rf_model_fit,
new_data = test_raw_data
)
final_metrics <- yardstick::metrics(
augment_test_data,
truth = detrend_close,
estimate = .pred
)
final_metrics
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp
)) +
geom_line(aes(
y = detrend_close,
color = "actual"
)) +
geom_line(aes(
y = .pred,
color = "predicted"
), linetype = "dashed")
parametric_cv <- feature_data %>%
mutate(
gamma_fit = pgamma(cv_daily,
shape = fit_g$estimate[1],
rate = fit_g$estimate[2]
),
gamma_fit_2 = gamma_fit^2,
gamma_fit_3 = gamma_fit^3
)
rf_data <- parametric_cv %>%
arrange(scaled_ts) %>%
select(
timestamp,
detrend_close,
scaled_ts,
gamma_fit,
gamma_fit_2,
gamma_fit_3,
sd_daily,
cv_daily,
open,
high,
low,
close
)
## training and test data -----
set.seed(420)
data_split <- initial_time_split(
rf_data,
prop = 0.8
)
train_raw_data <- training(data_split)
test_raw_data <- testing(data_split)
## feature engineering ----
lag_recipe <- recipe(
detrend_close ~ .,
data = train_raw_data
) %>%
step_lag(
timestamp,
gamma_fit,
gamma_fit_2,
gamma_fit_3,
close,
lag = 1:10,
prefix = "lag_"
) %>%
step_date(
timestamp,
features = c("month", "dow", "doy")
) %>%
step_normalize(all_numeric_predictors()) %>%
step_naomit(everything())
## random forest specification ----
rf_model_spec <- rand_forest(
mtry = tune(),
trees = 500
) %>%
set_engine("ranger") %>%
set_mode("regression")
## workflow ----
rf_workflow <- workflow() %>%
add_recipe(lag_recipe) %>%
add_model(rf_model_spec)
## time series cv ----
set.seed(69)
time_series_cv_folds <- rolling_origin(
train_raw_data,
initial = floor(nrow(train_raw_data) * 0.3),
asses = floor(nrow(train_raw_data) * 0.05),
skip = floor(nrow(train_raw_data) * 0.05),
cumulative = FALSE
)
## hyperparameter tuning grid ----
temp_prepped_recipe <- prep(
lag_recipe,
training = train_raw_data
)
num_predictor_approx <- length(
temp_prepped_recipe$term_info %>%
filter(role == "predictor") %>%
pull(variable)
)
mtry_upper_bound <- max(2, num_predictor_approx)
mtry_lower_bound <- 2
rf_param_grid <- grid_space_filling(
mtry(range = c(mtry_lower_bound, mtry_upper_bound)),
size = 5
)
## tunning process ----
set.seed(43)
control_tuning <- control_grid(
save_pred = TRUE,
verbose = TRUE,
parallel_over = "everything"
)
rf_tune_results <- tune_grid(
rf_workflow,
resamples = time_series_cv_folds,
grid = rf_param_grid,
metrics = metric_set(rmse, mae, rsq),
control = control_tuning
)
## analyze tunning results ----
show_best(
rf_tune_results,
metric = "rmse",
n = 5
)
autoplot(rf_tune_results)
best_rf_params <- select_best(
rf_tune_results,
metric = "rmse"
)
best_rf_params
## finalize workflow ----
final_rf_workflow <- finalize_workflow(
rf_workflow,
best_rf_params
)
final_rf_model_fit <- fit(
final_rf_workflow,
data = train_raw_data
)
## evaluate final model ----
processed_test_data_for_eval <- prep(
lag_recipe,
training = train_raw_data
) %>%
bake(new_data = test_raw_data)
augment_test_data <- augment(
final_rf_model_fit,
new_data = test_raw_data
)
final_metrics <- yardstick::metrics(
augment_test_data,
truth = detrend_close,
estimate = .pred
)
final_metrics
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp
)) +
geom_line(aes(
y = detrend_close,
color = "actual"
)) +
geom_line(aes(
y = .pred,
color = "predicted"
), linetype = "dashed")
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp
)) +
geom_line(aes(
y = scales::rescale(detrend_close, to = c(0, 1)),
color = "actual"
)) +
geom_line(aes(
y = scales::rescale(.pred, to = c(0, 1)),
color = "predicted"
), linetype = "dashed")
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp
)) +
geom_line(aes(
y = scales::rescale(detrend_close, to = c(0, 1)),
color = "actual"
)) +
geom_line(aes(
y = scales::rescale(.pred, to = c(0, 1)),
color = "predicted"
))
parametric_cv <- feature_data %>%
mutate(
gamma_fit = pgamma(cv_daily,
shape = fit_g$estimate[1],
rate = fit_g$estimate[2]
),
gamma_fit_2 = gamma_fit^2,
gamma_fit_3 = gamma_fit^3
)
# models ----
## original data ----
rf_data <- parametric_cv %>%
arrange(scaled_ts) %>%
select(
timestamp,
detrend_close,
scaled_ts,
gamma_fit,
sd_daily,
cv_daily,
open,
high,
low,
close
)
## training and test data -----
set.seed(420)
data_split <- initial_time_split(
rf_data,
prop = 0.8
)
train_raw_data <- training(data_split)
test_raw_data <- testing(data_split)
## feature engineering ----
lag_recipe <- recipe(
detrend_close ~ .,
data = train_raw_data
) %>%
step_lag(
timestamp,
gamma_fit,
close,
lag = 1:100,
prefix = "lag_"
) %>%
step_date(
timestamp,
features = c("month", "dow", "doy")
) %>%
step_normalize(all_numeric_predictors()) %>%
step_naomit(everything())
## random forest specification ----
rf_model_spec <- rand_forest(
mtry = tune(),
trees = 500
) %>%
set_engine("ranger") %>%
set_mode("regression")
## workflow ----
rf_workflow <- workflow() %>%
add_recipe(lag_recipe) %>%
add_model(rf_model_spec)
## time series cv ----
set.seed(69)
time_series_cv_folds <- rolling_origin(
train_raw_data,
initial = floor(nrow(train_raw_data) * 0.3),
asses = floor(nrow(train_raw_data) * 0.05),
skip = floor(nrow(train_raw_data) * 0.05),
cumulative = FALSE
)
## hyperparameter tuning grid ----
temp_prepped_recipe <- prep(
lag_recipe,
training = train_raw_data
)
num_predictor_approx <- length(
temp_prepped_recipe$term_info %>%
filter(role == "predictor") %>%
pull(variable)
)
mtry_upper_bound <- max(2, num_predictor_approx)
mtry_lower_bound <- 2
rf_param_grid <- grid_space_filling(
mtry(range = c(mtry_lower_bound, mtry_upper_bound)),
size = 5
)
## tunning process ----
set.seed(43)
control_tuning <- control_grid(
save_pred = TRUE,
verbose = TRUE,
parallel_over = "everything"
)
rf_tune_results <- tune_grid(
rf_workflow,
resamples = time_series_cv_folds,
grid = rf_param_grid,
metrics = metric_set(rmse, mae, rsq),
control = control_tuning
)
## analyze tunning results ----
show_best(
rf_tune_results,
metric = "rmse",
n = 5
)
autoplot(rf_tune_results)
best_rf_params <- select_best(
rf_tune_results,
metric = "rmse"
)
best_rf_params
## finalize workflow ----
final_rf_workflow <- finalize_workflow(
rf_workflow,
best_rf_params
)
final_rf_model_fit <- fit(
final_rf_workflow,
data = train_raw_data
)
## evaluate final model ----
processed_test_data_for_eval <- prep(
lag_recipe,
training = train_raw_data
) %>%
bake(new_data = test_raw_data)
augment_test_data <- augment(
final_rf_model_fit,
new_data = test_raw_data
)
final_metrics <- yardstick::metrics(
augment_test_data,
truth = detrend_close,
estimate = .pred
)
final_metrics
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp
)) +
geom_line(aes(
y = scales::rescale(detrend_close, to = c(0, 1)),
color = "actual"
)) +
geom_line(aes(
y = scales::rescale(.pred, to = c(0, 1)),
color = "predicted"
))
## feature engineering ----
lag_recipe <- recipe(
detrend_close ~ .,
data = train_raw_data
) %>%
step_lag(
timestamp,
gamma_fit,
lag = 1:100,
prefix = "lag_"
) %>%
step_date(
timestamp,
features = c("month", "dow", "doy")
) %>%
step_normalize(all_numeric_predictors()) %>%
step_naomit(everything())
## random forest specification ----
rf_model_spec <- rand_forest(
mtry = tune(),
trees = 500
) %>%
set_engine("ranger") %>%
set_mode("regression")
## workflow ----
rf_workflow <- workflow() %>%
add_recipe(lag_recipe) %>%
add_model(rf_model_spec)
## time series cv ----
set.seed(69)
time_series_cv_folds <- rolling_origin(
train_raw_data,
initial = floor(nrow(train_raw_data) * 0.3),
asses = floor(nrow(train_raw_data) * 0.05),
skip = floor(nrow(train_raw_data) * 0.05),
cumulative = FALSE
)
## hyperparameter tuning grid ----
temp_prepped_recipe <- prep(
lag_recipe,
training = train_raw_data
)
num_predictor_approx <- length(
temp_prepped_recipe$term_info %>%
filter(role == "predictor") %>%
pull(variable)
)
mtry_upper_bound <- max(2, num_predictor_approx)
mtry_lower_bound <- 2
rf_param_grid <- grid_space_filling(
mtry(range = c(mtry_lower_bound, mtry_upper_bound)),
size = 5
)
## tunning process ----
set.seed(43)
control_tuning <- control_grid(
save_pred = TRUE,
verbose = TRUE,
parallel_over = "everything"
)
rf_tune_results <- tune_grid(
rf_workflow,
resamples = time_series_cv_folds,
grid = rf_param_grid,
metrics = metric_set(rmse, mae, rsq),
control = control_tuning
)
## analyze tunning results ----
show_best(
rf_tune_results,
metric = "rmse",
n = 5
)
autoplot(rf_tune_results)
best_rf_params <- select_best(
rf_tune_results,
metric = "rmse"
)
best_rf_params
## finalize workflow ----
final_rf_workflow <- finalize_workflow(
rf_workflow,
best_rf_params
)
final_rf_model_fit <- fit(
final_rf_workflow,
data = train_raw_data
)
## evaluate final model ----
processed_test_data_for_eval <- prep(
lag_recipe,
training = train_raw_data
) %>%
bake(new_data = test_raw_data)
augment_test_data <- augment(
final_rf_model_fit,
new_data = test_raw_data
)
final_metrics <- yardstick::metrics(
augment_test_data,
truth = detrend_close,
estimate = .pred
)
final_metrics
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp
)) +
geom_line(aes(
y = scales::rescale(detrend_close, to = c(0, 1)),
color = "actual"
)) +
geom_line(aes(
y = scales::rescale(.pred, to = c(0, 1)),
color = "predicted"
))
augment_test_data
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp, y = dentrend_close - .pred
)) +
geom_line()
augment_test_data
augment_test_data %>%
filter(timestamp > "2024-01-01") %>%
ggplot(aes(
x = timestamp, y = detrend_close - .pred
)) +
geom_line()
